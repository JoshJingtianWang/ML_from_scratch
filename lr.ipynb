{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal equation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of the normal equation\n",
    "\n",
    "### 1. Model and cost function\n",
    "Suppose a linear model:\n",
    "$$\n",
    "h_\\theta(x) = \\theta^T x,\n",
    "$$\n",
    "\n",
    "Which expands to:\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n,\n",
    "$$\n",
    "\n",
    "Loss function (mean squared error):\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2,\n",
    "$$\n",
    "\n",
    "In matrix notation, this becomes:\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2} (X\\theta - y)^T (X\\theta - y)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the design matrix (m samples x n features).\n",
    "- $y$ is the column vector of output values.\n",
    "- $\\theta$ is the column vector of model parameters.\n",
    "\n",
    "### 2. Minimize the Cost Function\n",
    "To find the value of $\\theta$ that minimizes $J(\\theta)$, we'll take the derivative with respect to $\\theta$ and set it to zero.\n",
    "\n",
    "Differentiating $J(\\theta)$ with respect to $\\theta$:\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta} = X^T (X\\theta - y)$$\n",
    "\n",
    "For minimum, we'll set this to zero:\n",
    "$$X^T (X\\theta - y) = 0$$\n",
    "\n",
    "Expanding:\n",
    "$$X^T X \\theta = X^T y$$\n",
    "\n",
    "Now, to solve for $\\theta$:\n",
    "$$\\theta = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "This is the **normal equation**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_linear_regression(X, y):\n",
    "    \"\"\"\n",
    "    Fit multiple linear regression.\n",
    "    Parameters:\n",
    "    - X: 2D numpy array where each row is a data point and each column is a predictor variable.\n",
    "         It doesn't include the bias (intercept) term.\n",
    "    - y: 1D numpy array, the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - beta: Estimated coefficients including intercept.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add a bias column with all 1s to X\n",
    "    ones = np.ones(X.shape[0])\n",
    "    X = np.column_stack([ones, X])\n",
    "    \n",
    "    # Calculate coefficients using the normal equation\n",
    "    beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, beta):\n",
    "    \"\"\"\n",
    "    Predict using the fitted multiple linear regression model.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: 2D numpy array where each row is a data point and each column is a predictor variable.\n",
    "         It doesn't include the bias (intercept) term.\n",
    "    - beta: 1D numpy array, the coefficients from the multiple_linear_regression.\n",
    "    \n",
    "    Returns:\n",
    "    - Predictions as a 1D numpy array.\n",
    "    \"\"\"\n",
    "    \n",
    "    ones = np.ones(X.shape[0])\n",
    "    X = np.column_stack([ones, X])\n",
    "    \n",
    "    return X.dot(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: [-3. -1.  3.]\n",
      "Predictions: [ 2. 10.]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "X = np.array([[0, 2], [2, 3], [3, 4], [4, 5]])\n",
    "y = np.array([3, 4, 6, 8])\n",
    "beta = multiple_linear_regression(X, y)\n",
    "print(\"Final parameters:\", beta)\n",
    "\n",
    "X_new = np.array([[1, 2], [2, 5]])\n",
    "predictions = predict(X_new, beta)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent method\n",
    "\n",
    "When the derivatives are not directly solve-able, we can use gradient descent to approximate the global minimum of the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost (mean squared error) of using theta as the parameter for linear regression \n",
    "    to fit the data in X and y.\n",
    "    \n",
    "    Parameters:\n",
    "    - X : numpy.ndarray\n",
    "        The feature matrix. It includes the bias term (a column of ones).\n",
    "    - y : numpy.ndarray\n",
    "        The target variable vector.\n",
    "    - theta : numpy.ndarray\n",
    "        The parameter vector for linear regression.\n",
    "        \n",
    "    Returns:\n",
    "    - float\n",
    "        The cost computed using the provided theta.\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "    cost = (1/(2*m)) * np.sum(np.square(predictions - y))\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, y, alpha, num_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to learn theta parameters for linear regression.\n",
    "    \n",
    "    Parameters:\n",
    "    - X : numpy.ndarray\n",
    "        The feature matrix. It doesn't include the bias term.\n",
    "    - y : numpy.ndarray\n",
    "        The target variable vector.\n",
    "    - alpha : float\n",
    "        The learning rate.\n",
    "    - num_iterations : int\n",
    "        The number of iterations for gradient descent.\n",
    "        \n",
    "    Returns:\n",
    "    - tuple (theta, cost_history)\n",
    "        - theta : numpy.ndarray\n",
    "            The final parameter vector learned after gradient descent.\n",
    "        - cost_history : numpy.ndarray\n",
    "            The history of cost computed at each iteration.\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(num_iterations)\n",
    "    ones = np.ones(X.shape[0])\n",
    "    X = np.column_stack([ones, X])\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    for i in range(num_iterations):\n",
    "        gradients = (1/m) * X.T.dot(X.dot(theta) - y)\n",
    "        theta = theta - alpha * gradients\n",
    "        cost_history[i] = compute_cost(X, y, theta)\n",
    "        \n",
    "    return theta, cost_history\n",
    "\n",
    "def predict(X, theta):\n",
    "    ones = np.ones(X.shape[0])\n",
    "    X = np.column_stack([ones, X])\n",
    "    return X.dot(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: [-2.73837898 -0.8983058   2.86052068]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "X = np.array([[0, 2], [2, 3], [3, 4], [4, 5]])\n",
    "y = np.array([3, 4, 6, 8])\n",
    "\n",
    "# Initialize parameters\n",
    "alpha = 0.03  # learning rate\n",
    "num_iterations = 10000\n",
    "\n",
    "theta_final, cost_history = gradient_descent(X, y, alpha, num_iterations)\n",
    "print(\"Final parameters:\", theta_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.08435657 9.76761281]\n"
     ]
    }
   ],
   "source": [
    "X_new = np.array([[1, 2], [2, 5]])\n",
    "predictions = predict(X_new, theta_final)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
